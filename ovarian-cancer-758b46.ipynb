{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":45867,"databundleVersionId":6924515,"sourceType":"competition"},{"sourceId":3731,"sourceType":"modelInstanceVersion","modelInstanceId":2658,"modelId":312}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-16T19:59:56.918927Z","iopub.execute_input":"2024-08-16T19:59:56.919343Z","iopub.status.idle":"2024-08-16T19:59:57.397647Z","shell.execute_reply.started":"2024-08-16T19:59:56.919308Z","shell.execute_reply":"2024-08-16T19:59:57.396397Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom PIL import Image\nfrom scipy.signal import find_peaks, savgol_filter\nimport seaborn as sns\nimport os\nimport random\nimport cv2 as cv\nimport matplotlib.patches as patches\nimport re","metadata":{"execution":{"iopub.status.busy":"2024-08-16T19:59:57.399832Z","iopub.execute_input":"2024-08-16T19:59:57.400294Z","iopub.status.idle":"2024-08-16T19:59:59.532457Z","shell.execute_reply.started":"2024-08-16T19:59:57.400264Z","shell.execute_reply":"2024-08-16T19:59:59.530922Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Set random seed\nrandom.seed(41)\n\n# Set max pixels\nImage.MAX_IMAGE_PIXELS = None","metadata":{"execution":{"iopub.status.busy":"2024-08-16T19:59:59.534389Z","iopub.execute_input":"2024-08-16T19:59:59.534910Z","iopub.status.idle":"2024-08-16T19:59:59.544738Z","shell.execute_reply.started":"2024-08-16T19:59:59.534865Z","shell.execute_reply":"2024-08-16T19:59:59.541726Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Seaborn style\nsns.set()\nsns.set_theme(style=\"darkgrid\")\ncoolwarm = sns.color_palette(\"coolwarm\")\npastel = sns.color_palette(\"pastel\")","metadata":{"execution":{"iopub.status.busy":"2024-08-16T19:59:59.549822Z","iopub.execute_input":"2024-08-16T19:59:59.550441Z","iopub.status.idle":"2024-08-16T19:59:59.571992Z","shell.execute_reply.started":"2024-08-16T19:59:59.550372Z","shell.execute_reply":"2024-08-16T19:59:59.570678Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"BASE_PATH = '/kaggle/input/UBC-OCEAN'\nTRAIN_IMG_PATH = BASE_PATH + \"/train_images\"\nTRAIN_THUMB_PATH = BASE_PATH + \"/train_thumbnails\"\nTRAIN_ANN_PATH = BASE_PATH + \"/train.csv\"\nTEST_IMG_PATH = BASE_PATH + \"/test_thumbnails\"\nTEST_ANN_PATH = BASE_PATH + \"/test.csv\"","metadata":{"execution":{"iopub.status.busy":"2024-08-16T19:59:59.573427Z","iopub.execute_input":"2024-08-16T19:59:59.573814Z","iopub.status.idle":"2024-08-16T19:59:59.591022Z","shell.execute_reply.started":"2024-08-16T19:59:59.573780Z","shell.execute_reply":"2024-08-16T19:59:59.589507Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Train annotations overview\ntrain_df = pd.read_csv(TRAIN_ANN_PATH)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-16T19:59:59.592970Z","iopub.execute_input":"2024-08-16T19:59:59.594582Z","iopub.status.idle":"2024-08-16T19:59:59.647743Z","shell.execute_reply.started":"2024-08-16T19:59:59.594448Z","shell.execute_reply":"2024-08-16T19:59:59.646276Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   image_id label  image_width  image_height  is_tma\n0         4  HGSC        23785         20008   False\n1        66  LGSC        48871         48195   False\n2        91  HGSC         3388          3388    True\n3       281  LGSC        42309         15545   False\n4       286    EC        37204         30020   False","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>label</th>\n      <th>image_width</th>\n      <th>image_height</th>\n      <th>is_tma</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>HGSC</td>\n      <td>23785</td>\n      <td>20008</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>66</td>\n      <td>LGSC</td>\n      <td>48871</td>\n      <td>48195</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>91</td>\n      <td>HGSC</td>\n      <td>3388</td>\n      <td>3388</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>281</td>\n      <td>LGSC</td>\n      <td>42309</td>\n      <td>15545</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>286</td>\n      <td>EC</td>\n      <td>37204</td>\n      <td>30020</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def path_column(image_id, path=\"/kaggle/input/UBC-OCEAN/train_images/\"):\n    return path + str(image_id) + \".png\"\n\ntrain_df[\"file_path\"] = train_df[\"image_id\"].map(path_column)","metadata":{"execution":{"iopub.status.busy":"2024-08-16T19:59:59.649395Z","iopub.execute_input":"2024-08-16T19:59:59.649872Z","iopub.status.idle":"2024-08-16T19:59:59.659730Z","shell.execute_reply.started":"2024-08-16T19:59:59.649830Z","shell.execute_reply":"2024-08-16T19:59:59.658242Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport cv2\nimport math\nimport copy\nimport time\nimport random\nimport glob\nfrom matplotlib import pyplot as plt\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\nimport torchvision\n\n# Utils\nimport joblib\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\n# Sklearn Imports\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import balanced_accuracy_score, confusion_matrix, f1_score\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# For Image Models\nimport timm\n\n# Albumentations for augmentations\n# import albumentations as A\n# from albumentations.pytorch import ToTensorV2\n\n# For colored terminal text\nfrom colorama import Fore, Back, Style\nb_ = Fore.BLUE\nsr_ = Style.RESET_ALL\n\nimport warnings\n# warnings.filterwarnings(\"ignore\")\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"execution":{"iopub.status.busy":"2024-08-16T19:59:59.661275Z","iopub.execute_input":"2024-08-16T19:59:59.661684Z","iopub.status.idle":"2024-08-16T20:00:07.196827Z","shell.execute_reply.started":"2024-08-16T19:59:59.661652Z","shell.execute_reply":"2024-08-16T20:00:07.195375Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"CONFIG = {\n    \"is_submission\": False,\n    \"n_fold\": 5,\n    'fold': 1,\n    \"seed\": 42,\n    \"img_size\": 512,\n    \"crop_vertical\":True,\n    \"model_name\": \"tf_efficientnet_b0_ns\",\n    \"num_classes\": 5,\n    \"valid_batch_size\": 16,\n    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n    # \"model_path\": '/kaggle/input/efficientnetb0-training-crop-images/best_model_checkpoint2023-10-26_09-10-29.pth',\n    \"encoder_path\": \"/kaggle/input/tf-efficientnet/pytorch/tf-efficientnet-b2/1/tf_efficientnet_b2_aa-60c94f97.pth\"\n}","metadata":{"execution":{"iopub.status.busy":"2024-08-16T20:00:07.198675Z","iopub.execute_input":"2024-08-16T20:00:07.199164Z","iopub.status.idle":"2024-08-16T20:00:07.207374Z","shell.execute_reply.started":"2024-08-16T20:00:07.199119Z","shell.execute_reply":"2024-08-16T20:00:07.205927Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"ROOT_DIR = '/kaggle/input/UBC-OCEAN'\nTRAIN_DIR = '/kaggle/input/UBC-OCEAN/train_thumbnails'\nTEST_DIR = '/kaggle/input/UBC-OCEAN/test_thumbnails'\nALT_TEST_DIR = '/kaggle/input/UBC-OCEAN/test_images'\nTMA_TRAIN_DIR = '/kaggle/input/UBC-OCEAN/train_images'\n\n\ndef get_train_file_path(df_train_row):\n    if df_train_row.is_tma == False:\n        return f\"{TRAIN_DIR}/{df_train_row.image_id}_thumbnail.png\"\n    else:\n        return f\"{TMA_TRAIN_DIR}/{df_train_row.image_id}.png\"\n\n\n\ndef get_test_file_path(image_id):\n    if os.path.exists(f\"{TEST_DIR}/{image_id}_thumbnail.png\"):\n        return f\"{TEST_DIR}/{image_id}_thumbnail.png\"\n    else:\n        return f\"{ALT_TEST_DIR}/{image_id}.png\"\n","metadata":{"execution":{"iopub.status.busy":"2024-08-16T20:00:07.212543Z","iopub.execute_input":"2024-08-16T20:00:07.213073Z","iopub.status.idle":"2024-08-16T20:00:07.229115Z","shell.execute_reply.started":"2024-08-16T20:00:07.213027Z","shell.execute_reply":"2024-08-16T20:00:07.227584Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if \"best_model\" in filename:\n            print(dirname+\"/\"+filename)\n        continue","metadata":{"execution":{"iopub.status.busy":"2024-08-16T20:00:07.231154Z","iopub.execute_input":"2024-08-16T20:00:07.231680Z","iopub.status.idle":"2024-08-16T20:00:07.540610Z","shell.execute_reply.started":"2024-08-16T20:00:07.231631Z","shell.execute_reply":"2024-08-16T20:00:07.538716Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv(\"/kaggle/input/UBC-OCEAN/test.csv\")\ndf_test['file_path'] = df_test['image_id'].apply(get_test_file_path)\ndf_test[\"target_label\"] = 0 \ndf_test","metadata":{"execution":{"iopub.status.busy":"2024-08-16T20:00:07.542549Z","iopub.execute_input":"2024-08-16T20:00:07.543069Z","iopub.status.idle":"2024-08-16T20:00:07.571866Z","shell.execute_reply.started":"2024-08-16T20:00:07.543024Z","shell.execute_reply":"2024-08-16T20:00:07.570402Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"   image_id  image_width  image_height  \\\n0        41        28469         16987   \n\n                                           file_path  target_label  \n0  /kaggle/input/UBC-OCEAN/test_thumbnails/41_thu...             0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>image_width</th>\n      <th>image_height</th>\n      <th>file_path</th>\n      <th>target_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>41</td>\n      <td>28469</td>\n      <td>16987</td>\n      <td>/kaggle/input/UBC-OCEAN/test_thumbnails/41_thu...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"class UBCDataset(Dataset):\n    def __init__(self, df, transforms=None):\n        self.df = df\n        self.filenames = df.file_path.values\n        self.labels =  df.target_label.values\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_path = self.filenames[idx]\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        if CONFIG[\"crop_vertical\"]:\n            img = crop_vertical(img)\n        \n        # img = custom_center_crop_or_resize(img, (1024, 1024))\n        \n        if self.transforms:\n            img = self.transforms(image=img)[\"image\"]\n            \n        return {\n            \"image\": img,\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long)\n               }\n    \ndef crop_vertical(image):\n    \"\"\"\n    Function crops images if multiple slices contained and separated by black vertical background.\n    \"\"\"\n    vertical_sum = np.sum(image, axis=(0, 2))\n\n    # Identify the positions where the sum is zero\n    zero_positions = np.where(vertical_sum == 0)[0]\n\n    if len(zero_positions)==0:\n        cropped_images = [image]\n    else:\n        # If the image does not start with a black area, add index 0\n        if zero_positions[0] != 0:\n            zero_positions = np.insert(zero_positions, 0, 0)\n\n        # If the image does not end with a black area, add the image width\n        if zero_positions[-1] != image.shape[1] - 1:\n            zero_positions = np.append(zero_positions, image.shape[1] - 1)\n\n        start_idx = zero_positions[0]\n        cropped_images = []\n\n        for idx in range(1, len(zero_positions)):\n            end_idx = zero_positions[idx]\n            if end_idx - start_idx > 1:  # If the width of the cropped section is greater than 1\n                cropped = image[:, start_idx:end_idx]\n                # only include samples which are of min size\n                if cropped.shape[1]>200:  \n                    cropped_images.append(cropped)\n                    # cv2.imwrite(f\"{save_prefix}_{idx}.jpg\", cropped)\n            start_idx = end_idx\n            \n\ndef crop_vertical(image):\n    \"\"\"\n    Function crops images if multiple slices contained and separated by black vertical background.\n    \"\"\"\n    vertical_sum = np.sum(image, axis=(0, 2))\n\n    # Identify the positions where the sum is zero\n    zero_positions = np.where(vertical_sum == 0)[0]\n\n    if len(zero_positions)==0:\n        cropped_images = [image]\n    else:\n        # If the image does not start with a black area, add index 0\n        if zero_positions[0] != 0:\n            zero_positions = np.insert(zero_positions, 0, 0)\n\n        # If the image does not end with a black area, add the image width\n        if zero_positions[-1] != image.shape[1] - 1:\n            zero_positions = np.append(zero_positions, image.shape[1] - 1)\n\n        start_idx = zero_positions[0]\n        cropped_images = []\n\n        for idx in range(1, len(zero_positions)):\n            end_idx = zero_positions[idx]\n            if end_idx - start_idx > 1:  # If the width of the cropped section is greater than 1\n                cropped = image[:, start_idx:end_idx]\n                # only include samples which are of min size\n                if cropped.shape[1]>200:  \n                    cropped_images.append(cropped)\n                    # cv2.imwrite(f\"{save_prefix}_{idx}.jpg\", cropped)\n            start_idx = end_idx\n    final_crops = []\n    # remove black bars above/below the crops \n    for cropped in cropped_images:\n        horizontal_sum = np.sum(cropped, axis=(1, 2))\n        zero_positions = np.where(horizontal_sum == 0)[0]\n        img_ = np.delete(cropped, zero_positions, axis=0)\n        final_crops.append(img_)\n    if len(final_crops)==0:\n        return image\n    return final_crops[0]\n\n\ndef custom_center_crop_or_resize(image, crop_size):\n    # If both dimensions of the image are greater than or equal to the desired size, apply CenterCrop\n    if image.shape[0] >= crop_size[0] and image.shape[1] >= crop_size[1]:\n        return A.CenterCrop(crop_size[0], crop_size[1])(image=image)[\"image\"]\n    # Else, just resize the image to the desired size\n    else:\n        return A.Resize(crop_size[0], crop_size[1])(image=image)[\"image\"]","metadata":{"execution":{"iopub.status.busy":"2024-08-16T20:00:07.573769Z","iopub.execute_input":"2024-08-16T20:00:07.574269Z","iopub.status.idle":"2024-08-16T20:00:07.605878Z","shell.execute_reply.started":"2024-08-16T20:00:07.574224Z","shell.execute_reply":"2024-08-16T20:00:07.604139Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"data_transforms = {\n    \"train\": A.Compose([\n        A.RandomResizedCrop(CONFIG['img_size'], CONFIG['img_size'], scale=(0.8, 1.0)),\n        A.HorizontalFlip(p=0.5),\n        A.RandomBrightnessContrast(p=0.2),\n        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=15, p=0.2),\n        A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n        A.CoarseDropout(p=0.2),\n        A.Cutout(p=0.2),\n        A.Normalize(\n            mean=[0.485, 0.456, 0.406], \n            std=[0.229, 0.224, 0.225], \n            max_pixel_value=255.0, \n            p=1.0\n        ),\n        ToTensorV2()], p=1.),\n    \n    \"valid\": A.Compose([\n        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n        A.Normalize(\n            mean=[0.485, 0.456, 0.406], \n            std=[0.229, 0.224, 0.225], \n            max_pixel_value=255.0, \n            p=1.0\n        ),\n        ToTensorV2()], p=1.)\n}","metadata":{"execution":{"iopub.status.busy":"2024-08-16T20:00:07.607216Z","iopub.execute_input":"2024-08-16T20:00:07.607855Z","iopub.status.idle":"2024-08-16T20:00:07.633541Z","shell.execute_reply.started":"2024-08-16T20:00:07.607796Z","shell.execute_reply":"2024-08-16T20:00:07.631938Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/albumentations/augmentations/dropout/cutout.py:49: FutureWarning: Cutout has been deprecated. Please use CoarseDropout\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"class GeM(nn.Module):\n    def __init__(self, p=3, eps=1e-6):\n        super(GeM, self).__init__()\n        self.p = nn.Parameter(torch.ones(1)*p)\n        self.eps = eps\n\n    def forward(self, x):\n        return self.gem(x, p=self.p, eps=self.eps)\n        \n    def gem(self, x, p=3, eps=1e-6):\n        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n        \n    def __repr__(self):\n        return self.__class__.__name__ + \\\n                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n                ', ' + 'eps=' + str(self.eps) + ')'\n\nclass EfficientNetB0(nn.Module):\n    '''\n    EfficientNet B0 fine-tune.\n    '''\n    def __init__(self, model_name, num_classes, pretrained=True, checkpoint_path=None):\n        '''\n        Fine tune for EfficientNetB0\n        Args\n            n_classes : int - Number of classification categories.\n            learnable_modules : tuple - Names of the modules to fine-tune.\n        Return\n            \n        '''\n        super(EfficientNetB0, self).__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained, checkpoint_path=checkpoint_path)\n\n        in_features = self.model.classifier.in_features\n        self.model.classifier = nn.Identity()\n        self.model.global_pool = nn.Identity()\n        self.pooling = GeM()\n        self.linear = nn.Linear(in_features, num_classes)\n        self.softmax = nn.Softmax(dim=1)\n    \n    \n    def forward(self, images):\n        \"\"\"\n        Forward function for the fine-tuned model\n        Args\n            x: \n        Return\n            result\n        \"\"\"\n        features = self.model(images)\n        pooled_features = self.pooling(features).flatten(1)\n        output = self.linear(pooled_features)\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-08-16T20:00:07.635419Z","iopub.execute_input":"2024-08-16T20:00:07.636027Z","iopub.status.idle":"2024-08-16T20:00:07.652920Z","shell.execute_reply.started":"2024-08-16T20:00:07.635981Z","shell.execute_reply":"2024-08-16T20:00:07.651565Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\n","metadata":{"execution":{"iopub.status.busy":"2024-08-16T20:00:07.654616Z","iopub.execute_input":"2024-08-16T20:00:07.655413Z","iopub.status.idle":"2024-08-16T20:00:07.671287Z","shell.execute_reply.started":"2024-08-16T20:00:07.655364Z","shell.execute_reply":"2024-08-16T20:00:07.669827Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()","metadata":{"execution":{"iopub.status.busy":"2024-08-16T20:00:07.673136Z","iopub.execute_input":"2024-08-16T20:00:07.673705Z","iopub.status.idle":"2024-08-16T20:00:07.687458Z","shell.execute_reply.started":"2024-08-16T20:00:07.673659Z","shell.execute_reply":"2024-08-16T20:00:07.685967Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def predict_val_dataset(model, CONFIG, df_validate, TRAIN_DIR=None, val_size=1.0):\n    if not CONFIG[\"is_submission\"]:\n        valid_dataset = UBCDataset(df_validate, transforms=data_transforms[\"valid\"])\n        valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], \n                              num_workers=2, shuffle=False, pin_memory=True)\n\n        preds = []\n        labels_list = []\n        valid_acc = 0.0\n\n        with torch.no_grad():\n            bar = tqdm(enumerate(valid_loader), total=len(valid_loader))\n            for step, data in bar: \n                # print(step)\n                images = data['image'].to(CONFIG[\"device\"], dtype=torch.float)\n                labels = data['label'].to(CONFIG[\"device\"], dtype=torch.long)\n\n                batch_size = images.size(0)\n                outputs = model(images)\n                print(\"outputs\", outputs)\n                probs = F.softmax(outputs, dim=1)\n                print(\"probs\", probs)\n                _ , predicted = torch.max(probs, dim=1)\n                print(\"predicted\", predicted)\n                preds.append( predicted.detach().cpu().numpy() )\n                print(\"predicted.detach().cpu().numpy()\", predicted.detach().cpu().numpy())\n                labels_list.append( labels.detach().cpu().numpy() )\n                acc = torch.sum( predicted == labels )\n                valid_acc  += acc.item()\n        valid_acc /= len(valid_loader.dataset)\n        print(\"preds\", preds)\n        preds = np.concatenate([label_encoder.inverse_transform(batch) for batch in preds])\n        labels_list = np.concatenate(labels_list).flatten()\n\n        \n        # Calculate Balanced Accuracy\n        bal_acc = balanced_accuracy_score(labels_list, preds)\n        # Calculate Confusion Matrix\n        conf_matrix = confusion_matrix(labels_list, preds)\n        macro_f1 = f1_score(labels_list, preds, average='macro')\n        micro_f1 = f1_score(labels_list, preds, average='micro')\n        weighted_f1 = f1_score(labels_list, preds, average='weighted')\n    \n        print(f\"Validation Accuracy: {valid_acc}\")\n        print(f\"Balanced Accuracy: {bal_acc}\")\n        print(f\"Macro F1-Score: {macro_f1}\")\n        print(f\"Micro F1-Score: {micro_f1}\")\n        print(f\"Weighted F1-Score: {weighted_f1}\")\n        print(f\"Confusion Matrix: {conf_matrix}\")\n        \n        # add to validation dataframe\n        df_validate[\"pred\"] = preds\n        df_validate[\"pred_labels\"] = pred_labels\n        return df_validate, preds, labels_list\n    else:\n        print(\"Skip validation on training set due to submission!\")\n        return None","metadata":{"execution":{"iopub.status.busy":"2024-08-16T20:00:07.689810Z","iopub.execute_input":"2024-08-16T20:00:07.690374Z","iopub.status.idle":"2024-08-16T20:00:07.711399Z","shell.execute_reply.started":"2024-08-16T20:00:07.690330Z","shell.execute_reply":"2024-08-16T20:00:07.709800Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import timm\nmodel = timm.create_model('tf_efficientnet_b2', pretrained = True,checkpoint_path='/kaggle/input/tf-efficientnet/pytorch/tf-efficientnet-b2/1/tf_efficientnet_b2_aa-60c94f97.pth')","metadata":{"execution":{"iopub.status.busy":"2024-08-16T20:00:07.713483Z","iopub.execute_input":"2024-08-16T20:00:07.714143Z","iopub.status.idle":"2024-08-16T20:00:09.362917Z","shell.execute_reply.started":"2024-08-16T20:00:07.714093Z","shell.execute_reply":"2024-08-16T20:00:09.361371Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/36.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a721740b9dc4973b10160b384eb8d70"}},"metadata":{}}]},{"cell_type":"code","source":"if not CONFIG[\"is_submission\"]: \n    df_train = pd.read_csv(\"/kaggle/input/UBC-OCEAN/train.csv\")\n    print(df_train.shape)\n    df_train['file_path'] = df_train.apply(lambda row: get_train_file_path(row), axis=1)\n    df_train['target_label'] = label_encoder.fit_transform(df_train['label'])\n    # use stratified K Fold for crossvalidation \n    skf = StratifiedKFold(n_splits=CONFIG['n_fold'], shuffle=True, random_state=CONFIG[\"seed\"])\n    for fold, ( _, val_) in enumerate(skf.split(X=df_train, y=df_train.target_label)):\n        df_train.loc[val_ , \"kfold\"] = int(fold)\n        \n    all_labels = []\n    all_predictions = []\n#     for fold, model in enumerate(models):\n    fold = 1\n    print(\"Evaluate Fold: \", fold)\n    \n    model.eval()\n    model.to(CONFIG[\"device\"])\n    df_train_fold = df_train[df_train[\"kfold\"]!=fold].reset_index(drop=True)\n    df_valid_fold = df_train[df_train[\"kfold\"]==fold].reset_index(drop=True)\n\n    #train_dataset = UBCDataset(df_train_fold, transforms=data_transforms[\"train\"])\n    #train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], \n    #                          num_workers=2, shuffle=False, pin_memory=True)\n    valid_dataset = UBCDataset(df_valid_fold, transforms=data_transforms[\"valid\"])\n    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], \n                              num_workers=2, shuffle=False, pin_memory=True)\n\n    df_validate, predictions, labels = predict_val_dataset(model, CONFIG, df_valid_fold, TRAIN_DIR, val_size=1)\n    all_labels.extend(labels)\n    all_predictions.extend(predictions)\n    display(df_validate)","metadata":{"execution":{"iopub.status.busy":"2024-08-16T20:00:09.364703Z","iopub.execute_input":"2024-08-16T20:00:09.365176Z","iopub.status.idle":"2024-08-16T20:01:22.978525Z","shell.execute_reply.started":"2024-08-16T20:00:09.365131Z","shell.execute_reply":"2024-08-16T20:01:22.976822Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"(538, 5)\nEvaluate Fold:  1\n","output_type":"stream"},{"name":"stderr","text":" 14%|█▍        | 1/7 [00:19<01:58, 19.74s/it]","output_type":"stream"},{"name":"stdout","text":"outputs tensor([[-0.9517, -0.1615, -0.7614,  ...,  2.0117, -0.7561,  0.2888],\n        [-1.5231, -0.7579,  0.4153,  ...,  1.1984, -0.8694,  0.1046],\n        [-1.3980, -0.3548, -0.7353,  ...,  0.4304, -0.3972,  0.2442],\n        ...,\n        [-1.2291, -0.3650, -0.7254,  ..., -1.0482,  0.0654,  1.5473],\n        [-1.3988, -0.9760, -1.2871,  ...,  1.3922, -0.0301,  2.1638],\n        [-0.9439, -0.8932, -1.0316,  ...,  0.0856, -0.2255,  0.3678]])\nprobs tensor([[1.0263e-04, 2.2618e-04, 1.2415e-04,  ..., 1.9872e-03, 1.2480e-04,\n         3.5483e-04],\n        [7.0817e-05, 1.5221e-04, 4.9202e-04,  ..., 1.0767e-03, 1.3616e-04,\n         3.6059e-04],\n        [3.8944e-05, 1.1054e-04, 7.5552e-05,  ..., 2.4238e-04, 1.0594e-04,\n         2.0121e-04],\n        ...,\n        [1.0297e-04, 2.4435e-04, 1.7041e-04,  ..., 1.2340e-04, 3.7576e-04,\n         1.6538e-03],\n        [5.3695e-05, 8.1949e-05, 6.0037e-05,  ..., 8.7511e-04, 2.1103e-04,\n         1.8929e-03],\n        [6.1885e-05, 6.5099e-05, 5.6686e-05,  ..., 1.7326e-04, 1.2693e-04,\n         2.2972e-04]])\npredicted tensor([111, 980, 824, 885, 824, 591, 938, 885, 529, 793, 649, 938, 793, 911,\n        658, 936])\npredicted.detach().cpu().numpy() [111 980 824 885 824 591 938 885 529 793 649 938 793 911 658 936]\n","output_type":"stream"},{"name":"stderr","text":" 29%|██▊       | 2/7 [00:30<01:11, 14.27s/it]","output_type":"stream"},{"name":"stdout","text":"outputs tensor([[-2.1068, -0.1590,  0.6800,  ..., -0.6296,  0.0458,  0.4443],\n        [-1.2023, -0.1873, -0.6073,  ...,  1.5965,  0.4899,  0.1716],\n        [-1.2637,  0.5375, -1.2529,  ...,  0.4702, -0.6430, -0.4114],\n        ...,\n        [-0.5984,  0.2889, -0.6824,  ..., -0.7873, -1.5014,  1.0161],\n        [-0.7485, -0.2368, -1.0035,  ...,  0.4255, -0.8885,  2.0216],\n        [-0.8583, -0.9881, -1.2434,  ...,  1.3039,  0.1728,  1.0047]])\nprobs tensor([[2.8463e-05, 1.9961e-04, 4.6191e-04,  ..., 1.2468e-04, 2.4498e-04,\n         3.6494e-04],\n        [4.6395e-05, 1.2802e-04, 8.4111e-05,  ..., 7.6205e-04, 2.5197e-04,\n         1.8328e-04],\n        [4.7359e-05, 2.8686e-04, 4.7878e-05,  ..., 2.6818e-04, 8.8106e-05,\n         1.1106e-04],\n        ...,\n        [1.2347e-04, 2.9987e-04, 1.1353e-04,  ..., 1.0222e-04, 5.0050e-05,\n         6.2049e-04],\n        [8.0123e-05, 1.3365e-04, 6.2085e-05,  ..., 2.5918e-04, 6.9653e-05,\n         1.2787e-03],\n        [6.1342e-05, 5.3877e-05, 4.1737e-05,  ..., 5.3310e-04, 1.7202e-04,\n         3.9525e-04]])\npredicted tensor([459, 911, 938, 735, 885, 459, 957, 885, 591, 885, 735, 911, 937, 824,\n        885, 735])\npredicted.detach().cpu().numpy() [459 911 938 735 885 459 957 885 591 885 735 911 937 824 885 735]\n","output_type":"stream"},{"name":"stderr","text":" 43%|████▎     | 3/7 [00:40<00:49, 12.32s/it]","output_type":"stream"},{"name":"stdout","text":"outputs tensor([[-1.1325e+00,  4.1118e-01, -1.3969e+00,  ...,  1.4721e-01,\n         -8.6265e-01,  4.1259e-01],\n        [-1.3503e+00, -5.0979e-01, -3.2036e-01,  ...,  5.6590e-01,\n         -6.1482e-01,  1.1800e+00],\n        [-7.1032e-01, -3.4617e-01, -6.1856e-01,  ...,  1.2924e-03,\n         -5.0743e-01,  1.1126e-02],\n        ...,\n        [-1.4775e+00,  1.9180e-01, -1.8301e+00,  ...,  1.6887e-01,\n         -1.2094e+00, -4.0562e-01],\n        [-4.6024e-01, -8.8987e-01, -1.2587e+00,  ...,  1.1396e+00,\n          4.4018e-02,  1.7745e-01],\n        [-1.1562e+00,  2.5555e-01, -2.3144e+00,  ...,  3.4312e-01,\n          1.7737e-01,  1.9754e-01]])\nprobs tensor([[8.8504e-05, 4.1434e-04, 6.7942e-05,  ..., 3.1821e-04, 1.1591e-04,\n         4.1492e-04],\n        [6.0876e-05, 1.4109e-04, 1.7051e-04,  ..., 4.1367e-04, 1.2702e-04,\n         7.6450e-04],\n        [1.1372e-04, 1.6368e-04, 1.2465e-04,  ..., 2.3168e-04, 1.3930e-04,\n         2.3397e-04],\n        ...,\n        [7.4317e-05, 3.9449e-04, 5.2232e-05,  ..., 3.8555e-04, 9.7164e-05,\n         2.1706e-04],\n        [1.0552e-04, 6.8665e-05, 4.7484e-05,  ..., 5.2253e-04, 1.7471e-04,\n         1.9965e-04],\n        [8.4644e-05, 3.4731e-04, 2.6581e-05,  ..., 3.7909e-04, 3.2119e-04,\n         3.2773e-04]])\npredicted tensor([885, 452, 936, 793, 936, 885, 885, 885, 936, 591, 885, 591, 885, 825,\n        911, 938])\npredicted.detach().cpu().numpy() [885 452 936 793 936 885 885 885 936 591 885 591 885 825 911 938]\n","output_type":"stream"},{"name":"stderr","text":" 57%|█████▋    | 4/7 [00:49<00:32, 11.00s/it]","output_type":"stream"},{"name":"stdout","text":"outputs tensor([[-1.0298, -0.4834, -1.0490,  ...,  0.0681, -0.2050,  0.7775],\n        [-1.3942,  0.6244, -1.5895,  ...,  0.4529, -0.3590,  0.3551],\n        [-1.6758, -0.2380, -0.5241,  ..., -0.0793, -0.7398,  0.4346],\n        ...,\n        [-1.2254, -0.3405, -0.5150,  ...,  0.2576, -0.2492,  0.8001],\n        [-1.6849, -1.0445, -0.0982,  ...,  0.1303,  0.4227,  1.2517],\n        [-1.3539, -0.5972,  0.0490,  ..., -0.6152, -0.9269,  0.9660]])\nprobs tensor([[5.2768e-05, 9.1135e-05, 5.1769e-05,  ..., 1.5819e-04, 1.2039e-04,\n         3.2157e-04],\n        [5.6477e-05, 4.2515e-04, 4.6457e-05,  ..., 3.5813e-04, 1.5902e-04,\n         3.2477e-04],\n        [3.7535e-05, 1.5807e-04, 1.1874e-04,  ..., 1.8526e-04, 9.5703e-05,\n         3.0971e-04],\n        ...,\n        [8.6861e-05, 2.1043e-04, 1.7674e-04,  ..., 3.8272e-04, 2.3054e-04,\n         6.5838e-04],\n        [3.2994e-05, 6.2600e-05, 1.6127e-04,  ..., 2.0268e-04, 2.7150e-04,\n         6.2200e-04],\n        [5.8899e-05, 1.2553e-04, 2.3953e-04,  ..., 1.2329e-04, 9.0264e-05,\n         5.9922e-04]])\npredicted tensor([911, 938, 938, 735, 885, 911, 937, 936, 115, 643, 980, 735, 936, 885,\n        824, 591])\npredicted.detach().cpu().numpy() [911 938 938 735 885 911 937 936 115 643 980 735 936 885 824 591]\n","output_type":"stream"},{"name":"stderr","text":" 71%|███████▏  | 5/7 [00:57<00:20, 10.14s/it]","output_type":"stream"},{"name":"stdout","text":"outputs tensor([[-0.8067,  0.2436,  0.2162,  ...,  1.0438, -0.4672,  1.9735],\n        [-0.7113,  0.0859, -1.3929,  ...,  1.2141, -0.8316,  0.7274],\n        [-1.3321,  0.5865, -0.1459,  ...,  0.4160, -0.5684,  0.1963],\n        ...,\n        [-1.8811,  0.5291, -0.7441,  ...,  0.0150, -1.1985,  1.4209],\n        [-0.8622, -0.2572, -0.5391,  ...,  0.9334, -0.3580,  0.4191],\n        [-1.0286,  0.0190,  0.2722,  ...,  0.9764, -0.2415,  1.6549]])\nprobs tensor([[1.1739e-04, 3.3554e-04, 3.2649e-04,  ..., 7.4694e-04, 1.6484e-04,\n         1.8925e-03],\n        [1.4217e-04, 3.1553e-04, 7.1911e-05,  ..., 9.7508e-04, 1.2606e-04,\n         5.9932e-04],\n        [5.8979e-05, 4.0174e-04, 1.9313e-04,  ..., 3.3876e-04, 1.2658e-04,\n         2.7193e-04],\n        ...,\n        [3.2965e-05, 3.6709e-04, 1.0276e-04,  ..., 2.1954e-04, 6.5236e-05,\n         8.9554e-04],\n        [1.1338e-04, 2.0762e-04, 1.5662e-04,  ..., 6.8285e-04, 1.8770e-04,\n         4.0830e-04],\n        [8.4131e-05, 2.3984e-04, 3.0896e-04,  ..., 6.2480e-04, 1.8485e-04,\n         1.2314e-03]])\npredicted tensor([824, 885, 868, 735, 735, 885, 938, 793, 911, 938, 885, 735, 885, 885,\n         69, 591])\npredicted.detach().cpu().numpy() [824 885 868 735 735 885 938 793 911 938 885 735 885 885  69 591]\n","output_type":"stream"},{"name":"stderr","text":" 86%|████████▌ | 6/7 [01:06<00:09,  9.61s/it]","output_type":"stream"},{"name":"stdout","text":"outputs tensor([[-1.6774, -0.6642,  0.2020,  ...,  0.9542, -0.5470,  0.7588],\n        [-1.9593, -0.2336, -1.0884,  ..., -1.0418, -1.4322,  0.3138],\n        [-1.2798,  0.3062, -1.2653,  ...,  0.7059, -0.9658, -0.0644],\n        ...,\n        [-0.9782, -0.6225, -1.0451,  ...,  0.5818, -0.3006,  1.1562],\n        [-0.7867, -0.0089, -1.0390,  ...,  1.3397,  0.1183,  1.3414],\n        [-1.3858, -0.2589,  0.2762,  ..., -0.1039, -0.6444,  1.2557]])\nprobs tensor([[2.4232e-05, 6.6740e-05, 1.5870e-04,  ..., 3.3671e-04, 7.5039e-05,\n         2.7696e-04],\n        [3.8467e-05, 2.1604e-04, 9.1898e-05,  ..., 9.6285e-05, 6.5164e-05,\n         3.7347e-04],\n        [5.8959e-05, 2.8797e-04, 5.9822e-05,  ..., 4.2946e-04, 8.0711e-05,\n         1.9879e-04],\n        ...,\n        [8.8157e-05, 1.2583e-04, 8.2458e-05,  ..., 4.1952e-04, 1.7360e-04,\n         7.4511e-04],\n        [1.0991e-04, 2.3925e-04, 8.5405e-05,  ..., 9.2159e-04, 2.7170e-04,\n         9.2308e-04],\n        [3.7539e-05, 1.1585e-04, 1.9783e-04,  ..., 1.3528e-04, 7.8790e-05,\n         5.2685e-04]])\npredicted tensor([452, 643, 591, 643, 885, 885, 793, 735, 957, 980, 936, 452, 885, 824,\n        936, 591])\npredicted.detach().cpu().numpy() [452 643 591 643 885 885 793 735 957 980 936 452 885 824 936 591]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7/7 [01:12<00:00, 10.35s/it]","output_type":"stream"},{"name":"stdout","text":"outputs tensor([[-1.5157, -0.5443, -0.5928,  ...,  0.8604, -0.4771,  1.0077],\n        [-0.7544, -0.1305, -1.4716,  ...,  0.3403, -0.4963,  0.6633],\n        [-0.4770, -0.7931, -0.7533,  ...,  1.8620,  0.3759,  0.4979],\n        ...,\n        [-0.5093, -0.4928, -0.1744,  ..., -0.3098,  0.5992,  1.0548],\n        [-1.9054,  0.3613, -1.0241,  ...,  0.8766, -0.4535,  1.3802],\n        [-1.4771, -0.5835, -0.3725,  ..., -0.3096, -0.3376,  0.2507]])\nprobs tensor([[3.9052e-05, 1.0316e-04, 9.8271e-05,  ..., 4.2028e-04, 1.1033e-04,\n         4.8699e-04],\n        [1.2418e-04, 2.3175e-04, 6.0620e-05,  ..., 3.7111e-04, 1.6076e-04,\n         5.1257e-04],\n        [1.0660e-04, 7.7708e-05, 8.0856e-05,  ..., 1.1055e-03, 2.5012e-04,\n         2.8257e-04],\n        ...,\n        [1.5670e-04, 1.5932e-04, 2.1905e-04,  ..., 1.9131e-04, 4.7479e-04,\n         7.4882e-04],\n        [3.5003e-05, 3.3770e-04, 8.4498e-05,  ..., 5.6535e-04, 1.4951e-04,\n         9.3547e-04],\n        [3.9557e-05, 9.6674e-05, 1.1938e-04,  ..., 1.2714e-04, 1.2362e-04,\n         2.2263e-04]])\npredicted tensor([911, 735, 911, 937, 911, 496, 735, 885, 937, 911, 885, 868])\npredicted.detach().cpu().numpy() [911 735 911 937 911 496 735 885 937 911 885 868]\npreds [array([111, 980, 824, 885, 824, 591, 938, 885, 529, 793, 649, 938, 793,\n       911, 658, 936]), array([459, 911, 938, 735, 885, 459, 957, 885, 591, 885, 735, 911, 937,\n       824, 885, 735]), array([885, 452, 936, 793, 936, 885, 885, 885, 936, 591, 885, 591, 885,\n       825, 911, 938]), array([911, 938, 938, 735, 885, 911, 937, 936, 115, 643, 980, 735, 936,\n       885, 824, 591]), array([824, 885, 868, 735, 735, 885, 938, 793, 911, 938, 885, 735, 885,\n       885,  69, 591]), array([452, 643, 591, 643, 885, 885, 793, 735, 957, 980, 936, 452, 885,\n       824, 936, 591]), array([911, 735, 911, 937, 911, 496, 735, 885, 937, 911, 885, 868])]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m valid_dataset \u001b[38;5;241m=\u001b[39m UBCDataset(df_valid_fold, transforms\u001b[38;5;241m=\u001b[39mdata_transforms[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     26\u001b[0m valid_loader \u001b[38;5;241m=\u001b[39m DataLoader(valid_dataset, batch_size\u001b[38;5;241m=\u001b[39mCONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid_batch_size\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     27\u001b[0m                           num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 29\u001b[0m df_validate, predictions, labels \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_val_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_valid_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRAIN_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m all_labels\u001b[38;5;241m.\u001b[39mextend(labels)\n\u001b[1;32m     31\u001b[0m all_predictions\u001b[38;5;241m.\u001b[39mextend(predictions)\n","Cell \u001b[0;32mIn[18], line 32\u001b[0m, in \u001b[0;36mpredict_val_dataset\u001b[0;34m(model, CONFIG, df_validate, TRAIN_DIR, val_size)\u001b[0m\n\u001b[1;32m     30\u001b[0m valid_acc \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(valid_loader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreds\u001b[39m\u001b[38;5;124m\"\u001b[39m, preds)\n\u001b[0;32m---> 32\u001b[0m preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([label_encoder\u001b[38;5;241m.\u001b[39minverse_transform(batch) \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m preds])\n\u001b[1;32m     33\u001b[0m labels_list \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(labels_list)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Calculate Balanced Accuracy\u001b[39;00m\n","Cell \u001b[0;32mIn[18], line 32\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     30\u001b[0m valid_acc \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(valid_loader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreds\u001b[39m\u001b[38;5;124m\"\u001b[39m, preds)\n\u001b[0;32m---> 32\u001b[0m preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([\u001b[43mlabel_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m preds])\n\u001b[1;32m     33\u001b[0m labels_list \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(labels_list)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Calculate Balanced Accuracy\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:162\u001b[0m, in \u001b[0;36mLabelEncoder.inverse_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    160\u001b[0m diff \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msetdiff1d(y, np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_)))\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(diff):\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(diff))\n\u001b[1;32m    163\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(y)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_[y]\n","\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: [111 529 591 649 658 793 824 885 911 936 938 980]"],"ename":"ValueError","evalue":"y contains previously unseen labels: [111 529 591 649 658 793 824 885 911 936 938 980]","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}